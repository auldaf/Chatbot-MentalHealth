{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"2f2da09f0f0b4ec58ac7c606c984f218":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_5b021f93d1e7408d825237ecb8fccb26","IPY_MODEL_09b4fa580aed4f10ac05672787f75ff6","IPY_MODEL_a404db4128aa4dea99acf3b10809c58e","IPY_MODEL_159530706584453d9143752082576c01"],"layout":"IPY_MODEL_8e7178a1e8404cd79b89fc42b851d3ea"}},"4f1b4f21bf8041ef82b6ae69596057e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f2eba73776044738f7668eddd9aa429","placeholder":"​","style":"IPY_MODEL_8e03712590ce41fd8cd35d42fcde8f55","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a74618330d334a4da0b9c39b84861042":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_dc6f4b58b9ff4024855cb8ab37054134","placeholder":"​","style":"IPY_MODEL_070f90acd2574e22b73cdd6f9f8a5b6a","value":""}},"c9c1a61ce7b84ea7a64cf9db4726b0fa":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_f23f4ed085df499393e45e77b09c4bbd","style":"IPY_MODEL_916d97fabe7b47c0ac38d0507a6a6ab6","value":true}},"30c341ec1dab45c7bbe5d2dcfc512aca":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_d23690c516ec4b18854d5186831f97f6","style":"IPY_MODEL_59bbe7c40e774dafb0ac88150184ab49","tooltip":""}},"1f5b824acfc94fb5904cc2629252f154":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a91792c02374a0ebb4827a5666bf3e9","placeholder":"​","style":"IPY_MODEL_dbb3afa347bd47ac86b5ff73573f23a8","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"8e7178a1e8404cd79b89fc42b851d3ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"3f2eba73776044738f7668eddd9aa429":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e03712590ce41fd8cd35d42fcde8f55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc6f4b58b9ff4024855cb8ab37054134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"070f90acd2574e22b73cdd6f9f8a5b6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f23f4ed085df499393e45e77b09c4bbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"916d97fabe7b47c0ac38d0507a6a6ab6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d23690c516ec4b18854d5186831f97f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59bbe7c40e774dafb0ac88150184ab49":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"2a91792c02374a0ebb4827a5666bf3e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbb3afa347bd47ac86b5ff73573f23a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bf36b89480b4fb7bdca75c0ce3a2414":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_916e34efa2a24c90bb7a7323cf657a56","placeholder":"​","style":"IPY_MODEL_634ab2b566cc4c08a5f04a1ed418c31b","value":"Connecting..."}},"916e34efa2a24c90bb7a7323cf657a56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"634ab2b566cc4c08a5f04a1ed418c31b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b021f93d1e7408d825237ecb8fccb26":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b7dc7a78a2c40e8b0ddc0daef34eb95","placeholder":"​","style":"IPY_MODEL_86510bc3c684411694742d44a6c2bb1e","value":"Token is valid (permission: fineGrained)."}},"09b4fa580aed4f10ac05672787f75ff6":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c8c7137e6b04b859778ef80ca719e96","placeholder":"​","style":"IPY_MODEL_6ea3a6e3c2db4a1490d0805f96c2ef7a","value":"Your token has been saved in your configured git credential helpers (store)."}},"a404db4128aa4dea99acf3b10809c58e":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45cfe6baebbe4ef99b9644559f26eff5","placeholder":"​","style":"IPY_MODEL_26ba817e02e8438daf5fcaa1339b5b47","value":"Your token has been saved to /root/.cache/huggingface/token"}},"159530706584453d9143752082576c01":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_291189c30dc54caf852811f9b7885a13","placeholder":"​","style":"IPY_MODEL_03dfb4f26ce440709afaf48ea72e3dc5","value":"Login successful"}},"7b7dc7a78a2c40e8b0ddc0daef34eb95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86510bc3c684411694742d44a6c2bb1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c8c7137e6b04b859778ef80ca719e96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ea3a6e3c2db4a1490d0805f96c2ef7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45cfe6baebbe4ef99b9644559f26eff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ba817e02e8438daf5fcaa1339b5b47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"291189c30dc54caf852811f9b7885a13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03dfb4f26ce440709afaf48ea72e3dc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3592567aea8846efb2421ddcaecb4a45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44cf1589660b4ab2b519b2e1ce375258","IPY_MODEL_05df420157db4b5997da297978bd309e","IPY_MODEL_a2041bce81d7449db3f191deacadfc7a"],"layout":"IPY_MODEL_61c82e2004a9482b9981b2e65842a34b"}},"44cf1589660b4ab2b519b2e1ce375258":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64ee94d7f4b249c08cebb372a3be0c84","placeholder":"​","style":"IPY_MODEL_c7314e4b787442c2b64a88bcfa1e2b9b","value":"Loading checkpoint shards:   0%"}},"05df420157db4b5997da297978bd309e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_48663d1f25ef46dd8bf28d396d2c6e77","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7a60c7e5ff54983879880536e5a9bb0","value":0}},"a2041bce81d7449db3f191deacadfc7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_473fffff326e44f996cb1cf46b067708","placeholder":"​","style":"IPY_MODEL_71b039dab8ef414995e88f7b9a44f527","value":" 0/2 [00:00&lt;?, ?it/s]"}},"61c82e2004a9482b9981b2e65842a34b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64ee94d7f4b249c08cebb372a3be0c84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7314e4b787442c2b64a88bcfa1e2b9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48663d1f25ef46dd8bf28d396d2c6e77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7a60c7e5ff54983879880536e5a9bb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"473fffff326e44f996cb1cf46b067708":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71b039dab8ef414995e88f7b9a44f527":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd5b26ce29ce4856b54b4c0e3bd072b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f905350807a4c2f891b6908b7e4e37f","IPY_MODEL_6d30927914954064a2daad27ac6b52cb","IPY_MODEL_f6b45984bb5c427f809e7567f564e6cc"],"layout":"IPY_MODEL_8c9bec9e38424947a2465226c4e590ea"}},"2f905350807a4c2f891b6908b7e4e37f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c27ef0c7f434d67a78c59bbe5d98ed8","placeholder":"​","style":"IPY_MODEL_bd445667019847cea940f156c2d3fb32","value":"Loading checkpoint shards: 100%"}},"6d30927914954064a2daad27ac6b52cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53288144254b4a40abe4e48842c6499e","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d89af08437024bd984f6605baafd3009","value":4}},"f6b45984bb5c427f809e7567f564e6cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6a85abe34f6455b935018fbea7d62d8","placeholder":"​","style":"IPY_MODEL_f4dea2b4e60148659dbe60e463e1460e","value":" 4/4 [01:20&lt;00:00, 26.88s/it]"}},"8c9bec9e38424947a2465226c4e590ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c27ef0c7f434d67a78c59bbe5d98ed8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd445667019847cea940f156c2d3fb32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53288144254b4a40abe4e48842c6499e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d89af08437024bd984f6605baafd3009":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6a85abe34f6455b935018fbea7d62d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4dea2b4e60148659dbe60e463e1460e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9897699,"sourceType":"datasetVersion","datasetId":6079677}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install kaggle\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeULTFUxPZYa","outputId":"cf1cd637-20b2-4024-ffa8-18a384c582d4","execution":{"iopub.status.busy":"2024-11-28T15:08:01.903085Z","iopub.execute_input":"2024-11-28T15:08:01.903424Z","iopub.status.idle":"2024-11-28T15:08:01.908013Z","shell.execute_reply.started":"2024-11-28T15:08:01.903395Z","shell.execute_reply":"2024-11-28T15:08:01.907116Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:08:01.909358Z","iopub.execute_input":"2024-11-28T15:08:01.909622Z","iopub.status.idle":"2024-11-28T15:08:01.921333Z","shell.execute_reply.started":"2024-11-28T15:08:01.909598Z","shell.execute_reply":"2024-11-28T15:08:01.920414Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"1b\")\n\nsecret_label = \"HF Hub\"\nsecret_value = \"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\"\nlogin(token=secret_value)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234,"referenced_widgets":["2f2da09f0f0b4ec58ac7c606c984f218","4f1b4f21bf8041ef82b6ae69596057e7","a74618330d334a4da0b9c39b84861042","c9c1a61ce7b84ea7a64cf9db4726b0fa","30c341ec1dab45c7bbe5d2dcfc512aca","1f5b824acfc94fb5904cc2629252f154","8e7178a1e8404cd79b89fc42b851d3ea","3f2eba73776044738f7668eddd9aa429","8e03712590ce41fd8cd35d42fcde8f55","dc6f4b58b9ff4024855cb8ab37054134","070f90acd2574e22b73cdd6f9f8a5b6a","f23f4ed085df499393e45e77b09c4bbd","916d97fabe7b47c0ac38d0507a6a6ab6","d23690c516ec4b18854d5186831f97f6","59bbe7c40e774dafb0ac88150184ab49","2a91792c02374a0ebb4827a5666bf3e9","dbb3afa347bd47ac86b5ff73573f23a8","5bf36b89480b4fb7bdca75c0ce3a2414","916e34efa2a24c90bb7a7323cf657a56","634ab2b566cc4c08a5f04a1ed418c31b","5b021f93d1e7408d825237ecb8fccb26","09b4fa580aed4f10ac05672787f75ff6","a404db4128aa4dea99acf3b10809c58e","159530706584453d9143752082576c01","7b7dc7a78a2c40e8b0ddc0daef34eb95","86510bc3c684411694742d44a6c2bb1e","2c8c7137e6b04b859778ef80ca719e96","6ea3a6e3c2db4a1490d0805f96c2ef7a","45cfe6baebbe4ef99b9644559f26eff5","26ba817e02e8438daf5fcaa1339b5b47","291189c30dc54caf852811f9b7885a13","03dfb4f26ce440709afaf48ea72e3dc5"]},"id":"0f9e-uuVNi10","outputId":"b58a0b65-428f-4c26-9c7c-4fad4bfb531b","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:08:01.922918Z","iopub.execute_input":"2024-11-28T15:08:01.923207Z","iopub.status.idle":"2024-11-28T15:08:02.516411Z","shell.execute_reply.started":"2024-11-28T15:08:01.923182Z","shell.execute_reply":"2024-11-28T15:08:02.515667Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import whoami\n\ntry:\n    user_info = whoami()\n    print(f\"Logged in as: {user_info['name']}\")\nexcept Exception as e:\n    print(\"Not logged in. Please log in with your Hugging Face token.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:08:02.517268Z","iopub.execute_input":"2024-11-28T15:08:02.517497Z","iopub.status.idle":"2024-11-28T15:08:02.588606Z","shell.execute_reply.started":"2024-11-28T15:08:02.517475Z","shell.execute_reply":"2024-11-28T15:08:02.587848Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Logged in as: auldaf\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install datasets\n!pip install evaluate\n!pip install rouge_score\n!pip install loralib\n!pip install accelerate\n!pip install bitsandbytes\n!pip install trl\n!pip install peft\n# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SiyZMU0BOKyO","outputId":"d96eb2aa-c6df-48dd-8e6b-1f7276d711d4","execution":{"iopub.status.busy":"2024-11-28T15:08:02.590788Z","iopub.execute_input":"2024-11-28T15:08:02.591379Z","iopub.status.idle":"2024-11-28T15:09:27.210349Z","shell.execute_reply.started":"2024-11-28T15:08:02.591341Z","shell.execute_reply":"2024-11-28T15:09:27.209417Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a38cc867580607b1057699c32eeb80a42f294cae15ea527d4b3df24fd112b639\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nCollecting loralib\n  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\nDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\nInstalling collected packages: loralib\nSuccessfully installed loralib-0.1.2\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nCollecting trl\n  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (0.34.2)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nCollecting transformers>=4.46.0 (from trl)\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.20.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nDownloading trl-0.12.1-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, trl\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.46.3 trl-0.12.1\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport pandas as pd\nimport re\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nimport evaluate\n","metadata":{"id":"LUTR5WM0OeTq","execution":{"iopub.status.busy":"2024-11-28T15:09:27.211564Z","iopub.execute_input":"2024-11-28T15:09:27.211882Z","iopub.status.idle":"2024-11-28T15:09:46.101090Z","shell.execute_reply.started":"2024-11-28T15:09:27.211845Z","shell.execute_reply":"2024-11-28T15:09:46.100203Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"c065N0iwOmD6","execution":{"iopub.status.busy":"2024-11-28T15:09:46.102207Z","iopub.execute_input":"2024-11-28T15:09:46.102721Z","iopub.status.idle":"2024-11-28T15:09:46.942641Z","shell.execute_reply.started":"2024-11-28T15:09:46.102693Z","shell.execute_reply":"2024-11-28T15:09:46.941946Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# !pip install torch --upgrade\n","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:09:46.943605Z","iopub.execute_input":"2024-11-28T15:09:46.943831Z","iopub.status.idle":"2024-11-28T15:09:46.947348Z","shell.execute_reply.started":"2024-11-28T15:09:46.943795Z","shell.execute_reply":"2024-11-28T15:09:46.946542Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import transformers\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"id":"SYeITDIJOoNr","execution":{"iopub.status.busy":"2024-11-28T15:16:51.090022Z","iopub.execute_input":"2024-11-28T15:16:51.090362Z","iopub.status.idle":"2024-11-28T15:16:51.094846Z","shell.execute_reply.started":"2024-11-28T15:16:51.090331Z","shell.execute_reply":"2024-11-28T15:16:51.093945Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"!pip install accelerate\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02Y6PZtlOqNC","outputId":"728429ed-d050-409b-d5cb-6245c702f2b2","execution":{"iopub.status.busy":"2024-11-28T15:09:46.958365Z","iopub.execute_input":"2024-11-28T15:09:46.958671Z","iopub.status.idle":"2024-11-28T15:09:55.206822Z","shell.execute_reply.started":"2024-11-28T15:09:46.958635Z","shell.execute_reply":"2024-11-28T15:09:55.205849Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install -i https://pypi.org/simple/ bitsandbytes\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11Nbwl-lOtbu","outputId":"5f245447-bdfd-4e56-c68d-fdcc004ac238","execution":{"iopub.status.busy":"2024-11-28T15:09:55.210134Z","iopub.execute_input":"2024-11-28T15:09:55.210467Z","iopub.status.idle":"2024-11-28T15:10:03.406981Z","shell.execute_reply.started":"2024-11-28T15:09:55.210433Z","shell.execute_reply":"2024-11-28T15:10:03.405875Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple/\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# !pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:10:03.408426Z","iopub.execute_input":"2024-11-28T15:10:03.408701Z","iopub.status.idle":"2024-11-28T15:10:03.412854Z","shell.execute_reply.started":"2024-11-28T15:10:03.408673Z","shell.execute_reply":"2024-11-28T15:10:03.412021Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# !pip install --upgrade transformers\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hItm4UosWjL","outputId":"13341f43-22b3-49ae-fe4a-2e7ed941b201","execution":{"iopub.status.busy":"2024-11-28T15:10:03.414048Z","iopub.execute_input":"2024-11-28T15:10:03.414309Z","iopub.status.idle":"2024-11-28T15:10:03.429184Z","shell.execute_reply.started":"2024-11-28T15:10:03.414284Z","shell.execute_reply":"2024-11-28T15:10:03.428399Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\n# from transformers import pipeline\n\n# messages = [\n#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n# ]\n# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\")\n# pipe(messages)\n\n# from transformers import pipeline\n\n# # Add your Hugging Face token here\n# token = \"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\"\n# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\", use_auth_token=token)\n","metadata":{"id":"mISj8KkwSW_Z","colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["3592567aea8846efb2421ddcaecb4a45","44cf1589660b4ab2b519b2e1ce375258","05df420157db4b5997da297978bd309e","a2041bce81d7449db3f191deacadfc7a","61c82e2004a9482b9981b2e65842a34b","64ee94d7f4b249c08cebb372a3be0c84","c7314e4b787442c2b64a88bcfa1e2b9b","48663d1f25ef46dd8bf28d396d2c6e77","c7a60c7e5ff54983879880536e5a9bb0","473fffff326e44f996cb1cf46b067708","71b039dab8ef414995e88f7b9a44f527"]},"outputId":"6e143dc3-af10-4196-b82f-5c039298108b","execution":{"iopub.status.busy":"2024-11-28T15:10:03.430192Z","iopub.execute_input":"2024-11-28T15:10:03.430448Z","iopub.status.idle":"2024-11-28T15:10:03.437703Z","shell.execute_reply.started":"2024-11-28T15:10:03.430424Z","shell.execute_reply":"2024-11-28T15:10:03.436934Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# # tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n# # model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"meta-llama/Llama-3.2-3B-Instruct\",\n#     torch_dtype=\"float16\",\n#     device_map=\"auto\"\n# )\n","metadata":{"id":"7oArxIIISaR8","colab":{"base_uri":"https://localhost:8080/","height":191,"referenced_widgets":["bd5b26ce29ce4856b54b4c0e3bd072b3","2f905350807a4c2f891b6908b7e4e37f","6d30927914954064a2daad27ac6b52cb","f6b45984bb5c427f809e7567f564e6cc","8c9bec9e38424947a2465226c4e590ea","9c27ef0c7f434d67a78c59bbe5d98ed8","bd445667019847cea940f156c2d3fb32","53288144254b4a40abe4e48842c6499e","d89af08437024bd984f6605baafd3009","a6a85abe34f6455b935018fbea7d62d8","f4dea2b4e60148659dbe60e463e1460e"]},"outputId":"d1da9cba-ed37-4ed7-c6b6-8d27644a92ef","execution":{"iopub.status.busy":"2024-11-28T15:10:03.438680Z","iopub.execute_input":"2024-11-28T15:10:03.438933Z","iopub.status.idle":"2024-11-28T15:10:03.449125Z","shell.execute_reply.started":"2024-11-28T15:10:03.438910Z","shell.execute_reply":"2024-11-28T15:10:03.448293Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# # Set pad_token as end-of-sentence token\n# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"ULH8KMLp01KE","outputId":"4f9433f1-bbc7-48dd-99df-a7dd9eb8ebfc","execution":{"iopub.status.busy":"2024-11-28T15:10:03.450242Z","iopub.execute_input":"2024-11-28T15:10:03.450489Z","iopub.status.idle":"2024-11-28T15:10:03.457337Z","shell.execute_reply.started":"2024-11-28T15:10:03.450465Z","shell.execute_reply":"2024-11-28T15:10:03.456583Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# def print_number_of_trainable_model_parameters(model):\n#     trainable_model_params = 0\n#     all_model_params = 0\n#     for _, param in model.named_parameters():\n#         all_model_params += param.numel()\n#         if param.requires_grad:\n#             trainable_model_params += param.numel()\n#     return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\n# print(print_number_of_trainable_model_parameters(model))","metadata":{"id":"7HFiLkAy0zTn","execution":{"iopub.status.busy":"2024-11-28T15:10:03.458417Z","iopub.execute_input":"2024-11-28T15:10:03.458743Z","iopub.status.idle":"2024-11-28T15:10:03.465409Z","shell.execute_reply.started":"2024-11-28T15:10:03.458718Z","shell.execute_reply":"2024-11-28T15:10:03.464575Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# !pip install bitsandbytes-cuda\n","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:10:03.466277Z","iopub.execute_input":"2024-11-28T15:10:03.466531Z","iopub.status.idle":"2024-11-28T15:10:03.475505Z","shell.execute_reply.started":"2024-11-28T15:10:03.466507Z","shell.execute_reply":"2024-11-28T15:10:03.474641Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:10:03.476611Z","iopub.execute_input":"2024-11-28T15:10:03.476933Z","iopub.status.idle":"2024-11-28T15:10:03.485970Z","shell.execute_reply.started":"2024-11-28T15:10:03.476908Z","shell.execute_reply":"2024-11-28T15:10:03.485179Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!pip show torch","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:17:01.489414Z","iopub.execute_input":"2024-11-28T15:17:01.490183Z","iopub.status.idle":"2024-11-28T15:17:09.811596Z","shell.execute_reply.started":"2024-11-28T15:17:01.490150Z","shell.execute_reply":"2024-11-28T15:17:09.810636Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Name: torch\nVersion: 2.4.0\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nHome-page: https://pytorch.org/\nAuthor: PyTorch Team\nAuthor-email: packages@pytorch.org\nLicense: BSD-3\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\nRequired-by: accelerate, bitsandbytes, easyocr, fastai, kornia, peft, pytorch-ignite, pytorch-lightning, stable-baselines3, timm, torchaudio, torchmetrics, torchvision\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# from transformers import pipeline\n\n# messages = [\n#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n# ]\n# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\")\n# pipe(messages)","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:10:11.402912Z","iopub.execute_input":"2024-11-28T15:10:11.403201Z","iopub.status.idle":"2024-11-28T15:10:11.407860Z","shell.execute_reply.started":"2024-11-28T15:10:11.403171Z","shell.execute_reply":"2024-11-28T15:10:11.406743Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# # tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n# # model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"meta-llama/Llama-3.2-3B-Instruct\",\n#     torch_dtype=\"float16\",\n#     device_map=\"auto\"\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-28T15:10:11.409047Z","iopub.execute_input":"2024-11-28T15:10:11.409295Z","iopub.status.idle":"2024-11-28T15:10:11.466487Z","shell.execute_reply.started":"2024-11-28T15:10:11.409271Z","shell.execute_reply":"2024-11-28T15:10:11.465665Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n\ncompute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.1-8B-Instruct\",\n    quantization_config=quant_config,\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"id":"cV4b2ZPROxOI","colab":{"base_uri":"https://localhost:8080/","height":808},"outputId":"a67d78d5-a99b-41a9-b024-472f1ca546eb","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:17:14.219126Z","iopub.execute_input":"2024-11-28T15:17:14.220028Z","iopub.status.idle":"2024-11-28T15:17:57.511109Z","shell.execute_reply.started":"2024-11-28T15:17:14.219989Z","shell.execute_reply":"2024-11-28T15:17:57.510158Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3bdc2810c6749f9bb6f4b39ae6d9eec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b96671846da4189889b60d3c3dcb48a"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:18:04.653747Z","iopub.execute_input":"2024-11-28T15:18:04.654550Z","iopub.status.idle":"2024-11-28T15:18:04.658369Z","shell.execute_reply.started":"2024-11-28T15:18:04.654516Z","shell.execute_reply":"2024-11-28T15:18:04.657379Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:18:08.174731Z","iopub.execute_input":"2024-11-28T15:18:08.175115Z","iopub.status.idle":"2024-11-28T15:18:08.182979Z","shell.execute_reply.started":"2024-11-28T15:18:08.175083Z","shell.execute_reply":"2024-11-28T15:18:08.182156Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 1050939392\nall model parameters: 4540600320\npercentage of trainable model parameters: 23.15%\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"Amod/mental_health_counseling_conversations\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:18:19.075099Z","iopub.execute_input":"2024-11-28T15:18:19.075923Z","iopub.status.idle":"2024-11-28T15:18:21.750468Z","shell.execute_reply.started":"2024-11-28T15:18:19.075890Z","shell.execute_reply":"2024-11-28T15:18:21.749856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bb60e824084460398ff1ade6be6e4e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"combined_dataset.json:   0%|          | 0.00/4.79M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb28a5813d94e3b8937e66ed7d1fcec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dbcc032bcce4e8a9b3f824a3c362c76"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:18:25.833450Z","iopub.execute_input":"2024-11-28T15:18:25.834135Z","iopub.status.idle":"2024-11-28T15:18:25.840285Z","shell.execute_reply.started":"2024-11-28T15:18:25.834099Z","shell.execute_reply":"2024-11-28T15:18:25.839329Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Context', 'Response'],\n        num_rows: 3512\n    })\n})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"print('Count of rows in the data is:  ', len(data))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:18:55.568663Z","iopub.execute_input":"2024-11-28T15:18:55.569513Z","iopub.status.idle":"2024-11-28T15:18:55.610121Z","shell.execute_reply.started":"2024-11-28T15:18:55.569478Z","shell.execute_reply":"2024-11-28T15:18:55.609101Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount of rows in the data is:  \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m))\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"],"ename":"NameError","evalue":"name 'data' is not defined","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"ds.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:19:03.592420Z","iopub.execute_input":"2024-11-28T15:19:03.592761Z","iopub.status.idle":"2024-11-28T15:19:03.598468Z","shell.execute_reply.started":"2024-11-28T15:19:03.592729Z","shell.execute_reply":"2024-11-28T15:19:03.597458Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'train': (3512, 2)}"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"df1= ds['train'].to_pandas()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:19:10.453668Z","iopub.execute_input":"2024-11-28T15:19:10.454324Z","iopub.status.idle":"2024-11-28T15:19:10.466778Z","shell.execute_reply.started":"2024-11-28T15:19:10.454290Z","shell.execute_reply":"2024-11-28T15:19:10.465951Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"df1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:19:14.008482Z","iopub.execute_input":"2024-11-28T15:19:14.009276Z","iopub.status.idle":"2024-11-28T15:19:14.030774Z","shell.execute_reply.started":"2024-11-28T15:19:14.009242Z","shell.execute_reply":"2024-11-28T15:19:14.030082Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n0     I'm going through some things with my feelings...   \n1     I'm going through some things with my feelings...   \n2     I'm going through some things with my feelings...   \n3     I'm going through some things with my feelings...   \n4     I'm going through some things with my feelings...   \n...                                                 ...   \n3507  My grandson's step-mother sends him to school ...   \n3508  My boyfriend is in recovery from drug addictio...   \n3509  The birth mother attempted suicide several tim...   \n3510  I think adult life is making him depressed and...   \n3511  I just took a job that requires me to travel f...   \n\n                                               Response  \n0     If everyone thinks you're worthless, then mayb...  \n1     Hello, and thank you for your question and see...  \n2     First thing I'd suggest is getting the sleep y...  \n3     Therapy is essential for those that are feelin...  \n4     I first want to let you know that you are not ...  \n...                                                 ...  \n3507  Absolutely not! It is never in a child's best ...  \n3508  I'm sorry you have tension between you and you...  \n3509  The true answer is, \"no one can really say wit...  \n3510  How do you help yourself to believe you requir...  \n3511                           hmm this is a tough one!  \n\n[3512 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>If everyone thinks you're worthless, then mayb...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Hello, and thank you for your question and see...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>First thing I'd suggest is getting the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Therapy is essential for those that are feelin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>I first want to let you know that you are not ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3507</th>\n      <td>My grandson's step-mother sends him to school ...</td>\n      <td>Absolutely not! It is never in a child's best ...</td>\n    </tr>\n    <tr>\n      <th>3508</th>\n      <td>My boyfriend is in recovery from drug addictio...</td>\n      <td>I'm sorry you have tension between you and you...</td>\n    </tr>\n    <tr>\n      <th>3509</th>\n      <td>The birth mother attempted suicide several tim...</td>\n      <td>The true answer is, \"no one can really say wit...</td>\n    </tr>\n    <tr>\n      <th>3510</th>\n      <td>I think adult life is making him depressed and...</td>\n      <td>How do you help yourself to believe you requir...</td>\n    </tr>\n    <tr>\n      <th>3511</th>\n      <td>I just took a job that requires me to travel f...</td>\n      <td>hmm this is a tough one!</td>\n    </tr>\n  </tbody>\n</table>\n<p>3512 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import spacy\n\n# Pastikan spacy sudah terinstal dan model bahasa diunduh\n# pip install spacy\n# python -m spacy download en_core_web_sm\n\nnlp = spacy.load('en_core_web_sm')\n\ndef to_base_form_spacy(sentence):\n    doc = nlp(sentence)\n    return ' '.join([token.lemma_ for token in doc])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:25:35.570916Z","iopub.execute_input":"2024-11-28T15:25:35.571254Z","iopub.status.idle":"2024-11-28T15:25:36.196248Z","shell.execute_reply.started":"2024-11-28T15:25:35.571225Z","shell.execute_reply":"2024-11-28T15:25:36.195351Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"df2 = df1[['Context', 'Response']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:25:44.453776Z","iopub.execute_input":"2024-11-28T15:25:44.454148Z","iopub.status.idle":"2024-11-28T15:25:44.459314Z","shell.execute_reply.started":"2024-11-28T15:25:44.454119Z","shell.execute_reply":"2024-11-28T15:25:44.458358Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"df2['Context'] = df1['Context'].apply(to_base_form_spacy)\ndf2['Response'] = df1['Response'].apply(to_base_form_spacy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:25:47.852310Z","iopub.execute_input":"2024-11-28T15:25:47.852637Z","iopub.status.idle":"2024-11-28T15:27:59.333008Z","shell.execute_reply.started":"2024-11-28T15:25:47.852607Z","shell.execute_reply":"2024-11-28T15:27:59.332073Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"df2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:28:05.364802Z","iopub.execute_input":"2024-11-28T15:28:05.365676Z","iopub.status.idle":"2024-11-28T15:28:05.374597Z","shell.execute_reply.started":"2024-11-28T15:28:05.365643Z","shell.execute_reply":"2024-11-28T15:28:05.373744Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n0     I be go through some thing with my feeling and...   \n1     I be go through some thing with my feeling and...   \n2     I be go through some thing with my feeling and...   \n3     I be go through some thing with my feeling and...   \n4     I be go through some thing with my feeling and...   \n...                                                 ...   \n3507  my grandson 's step - mother send he to school...   \n3508  my boyfriend be in recovery from drug addictio...   \n3509  the birth mother attempt suicide several time ...   \n3510  I think adult life be make he depressed and we...   \n3511  I just take a job that require I to travel far...   \n\n                                               Response  \n0     if everyone think you be worthless , then mayb...  \n1     hello , and thank you for your question and se...  \n2     first thing I would suggest be get the sleep y...  \n3     therapy be essential for those that be feel de...  \n4     I first want to let you know that you be not a...  \n...                                                 ...  \n3507  absolutely not !   it be never in a child 's g...  \n3508  I be sorry you have tension between you and yo...  \n3509  the true answer be , \" no one can really say w...  \n3510  how do you help yourself to believe you requir...  \n3511                          hmm this be a tough one !  \n\n[3512 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>if everyone think you be worthless , then mayb...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>hello , and thank you for your question and se...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>first thing I would suggest be get the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>therapy be essential for those that be feel de...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>I first want to let you know that you be not a...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3507</th>\n      <td>my grandson 's step - mother send he to school...</td>\n      <td>absolutely not !   it be never in a child 's g...</td>\n    </tr>\n    <tr>\n      <th>3508</th>\n      <td>my boyfriend be in recovery from drug addictio...</td>\n      <td>I be sorry you have tension between you and yo...</td>\n    </tr>\n    <tr>\n      <th>3509</th>\n      <td>the birth mother attempt suicide several time ...</td>\n      <td>the true answer be , \" no one can really say w...</td>\n    </tr>\n    <tr>\n      <th>3510</th>\n      <td>I think adult life be make he depressed and we...</td>\n      <td>how do you help yourself to believe you requir...</td>\n    </tr>\n    <tr>\n      <th>3511</th>\n      <td>I just take a job that require I to travel far...</td>\n      <td>hmm this be a tough one !</td>\n    </tr>\n  </tbody>\n</table>\n<p>3512 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ndf2['Context']=df2['Context'].apply(lambda x: clean_text(x))\ndf2['Response']=df2['Response'].apply(lambda x: clean_text(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:32:42.508493Z","iopub.execute_input":"2024-11-28T15:32:42.509397Z","iopub.status.idle":"2024-11-28T15:32:43.141762Z","shell.execute_reply.started":"2024-11-28T15:32:42.509358Z","shell.execute_reply":"2024-11-28T15:32:43.140838Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"df2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:32:58.024187Z","iopub.execute_input":"2024-11-28T15:32:58.025016Z","iopub.status.idle":"2024-11-28T15:32:58.034650Z","shell.execute_reply.started":"2024-11-28T15:32:58.024981Z","shell.execute_reply":"2024-11-28T15:32:58.033646Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n0     i be go through some thing with my feeling and...   \n1     i be go through some thing with my feeling and...   \n2     i be go through some thing with my feeling and...   \n3     i be go through some thing with my feeling and...   \n4     i be go through some thing with my feeling and...   \n...                                                 ...   \n3507  my grandson s step  mother send he to school w...   \n3508  my boyfriend be in recovery from drug addictio...   \n3509  the birth mother attempt suicide several time ...   \n3510  i think adult life be make he depressed and we...   \n3511  i just take a job that require i to travel far...   \n\n                                               Response  \n0     if everyone think you be worthless  then maybe...  \n1     hello  and thank you for your question and see...  \n2     first thing i would suggest be get the sleep y...  \n3     therapy be essential for those that be feel de...  \n4     i first want to let you know that you be not a...  \n...                                                 ...  \n3507  absolutely not    it be never in a child s goo...  \n3508  i be sorry you have tension between you and yo...  \n3509  the true answer be   no one can really say wit...  \n3510  how do you help yourself to believe you requir...  \n3511                           hmm this be a tough one   \n\n[3512 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>if everyone think you be worthless  then maybe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>hello  and thank you for your question and see...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>first thing i would suggest be get the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>therapy be essential for those that be feel de...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>i first want to let you know that you be not a...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3507</th>\n      <td>my grandson s step  mother send he to school w...</td>\n      <td>absolutely not    it be never in a child s goo...</td>\n    </tr>\n    <tr>\n      <th>3508</th>\n      <td>my boyfriend be in recovery from drug addictio...</td>\n      <td>i be sorry you have tension between you and yo...</td>\n    </tr>\n    <tr>\n      <th>3509</th>\n      <td>the birth mother attempt suicide several time ...</td>\n      <td>the true answer be   no one can really say wit...</td>\n    </tr>\n    <tr>\n      <th>3510</th>\n      <td>i think adult life be make he depressed and we...</td>\n      <td>how do you help yourself to believe you requir...</td>\n    </tr>\n    <tr>\n      <th>3511</th>\n      <td>i just take a job that require i to travel far...</td>\n      <td>hmm this be a tough one</td>\n    </tr>\n  </tbody>\n</table>\n<p>3512 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"new_df = df2[['Context', 'Response']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:33:06.961214Z","iopub.execute_input":"2024-11-28T15:33:06.961538Z","iopub.status.idle":"2024-11-28T15:33:06.966785Z","shell.execute_reply.started":"2024-11-28T15:33:06.961512Z","shell.execute_reply":"2024-11-28T15:33:06.965867Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:33:11.392716Z","iopub.execute_input":"2024-11-28T15:33:11.393408Z","iopub.status.idle":"2024-11-28T15:33:11.403137Z","shell.execute_reply.started":"2024-11-28T15:33:11.393370Z","shell.execute_reply":"2024-11-28T15:33:11.402315Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n0     i be go through some thing with my feeling and...   \n1     i be go through some thing with my feeling and...   \n2     i be go through some thing with my feeling and...   \n3     i be go through some thing with my feeling and...   \n4     i be go through some thing with my feeling and...   \n...                                                 ...   \n3507  my grandson s step  mother send he to school w...   \n3508  my boyfriend be in recovery from drug addictio...   \n3509  the birth mother attempt suicide several time ...   \n3510  i think adult life be make he depressed and we...   \n3511  i just take a job that require i to travel far...   \n\n                                               Response  \n0     if everyone think you be worthless  then maybe...  \n1     hello  and thank you for your question and see...  \n2     first thing i would suggest be get the sleep y...  \n3     therapy be essential for those that be feel de...  \n4     i first want to let you know that you be not a...  \n...                                                 ...  \n3507  absolutely not    it be never in a child s goo...  \n3508  i be sorry you have tension between you and yo...  \n3509  the true answer be   no one can really say wit...  \n3510  how do you help yourself to believe you requir...  \n3511                           hmm this be a tough one   \n\n[3512 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>if everyone think you be worthless  then maybe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>hello  and thank you for your question and see...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>first thing i would suggest be get the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>therapy be essential for those that be feel de...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>i first want to let you know that you be not a...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3507</th>\n      <td>my grandson s step  mother send he to school w...</td>\n      <td>absolutely not    it be never in a child s goo...</td>\n    </tr>\n    <tr>\n      <th>3508</th>\n      <td>my boyfriend be in recovery from drug addictio...</td>\n      <td>i be sorry you have tension between you and yo...</td>\n    </tr>\n    <tr>\n      <th>3509</th>\n      <td>the birth mother attempt suicide several time ...</td>\n      <td>the true answer be   no one can really say wit...</td>\n    </tr>\n    <tr>\n      <th>3510</th>\n      <td>i think adult life be make he depressed and we...</td>\n      <td>how do you help yourself to believe you requir...</td>\n    </tr>\n    <tr>\n      <th>3511</th>\n      <td>i just take a job that require i to travel far...</td>\n      <td>hmm this be a tough one</td>\n    </tr>\n  </tbody>\n</table>\n<p>3512 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Misalkan new_df adalah DataFrame yang berisi kolom 'Context' dan 'Response'\nnew_df = df2[['Context', 'Response']]\n\n# Memisahkan data menjadi data latih (train) dan data uji (test)\ntrain_df, test_df = train_test_split(new_df, test_size=0.2, random_state=42)\n\n# Menampilkan contoh data latih dan uji\nprint(\"Train data:\")\nprint(train_df.head())\n\nprint(\"Test data:\")\nprint(test_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:33:18.058241Z","iopub.execute_input":"2024-11-28T15:33:18.059110Z","iopub.status.idle":"2024-11-28T15:33:18.069964Z","shell.execute_reply.started":"2024-11-28T15:33:18.059075Z","shell.execute_reply":"2024-11-28T15:33:18.069072Z"}},"outputs":[{"name":"stdout","text":"Train data:\n                                                Context  \\\n2659  we do not have sex a lot  i cheat when we argu...   \n1057  i do not know how to notice or express my feel...   \n2111  i start counsel  therapy in a few day  i be fr...   \n1565  i do not know how to tell someone how i feel a...   \n631   my daughter seem to be develop at a normal rat...   \n\n                                               Response  \n2659   hello  and thank you for your question  the q...  \n1057  well  then give yourself some credit for notic...  \n2111  people do cry in therapy sometimes  but it be ...  \n1565   practice make perfectsimply by express yourse...  \n631   good for you to know your daughter s friendshi...  \nTest data:\n                                                Context  \\\n495   i have hit my head on wall and floor ever sinc...   \n1592  over a year ago i have a female friend  she tu...   \n2314  my long  distance girlfriend be in a sorority ...   \n1475  cheating be something unacceptable for i but b...   \n2772  i have twin toddler  i experience a death of l...   \n\n                                               Response  \n495   the good way to handle anxiety of this level b...  \n1592  we woman really do tend to struggle with the c...  \n2314  you may already be do as much as possible for ...  \n1475  it be completely understandable that you be st...  \n2772  first  let i say that you be a survivor and a ...  \n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"len(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:33:26.126830Z","iopub.execute_input":"2024-11-28T15:33:26.127734Z","iopub.status.idle":"2024-11-28T15:33:26.133517Z","shell.execute_reply.started":"2024-11-28T15:33:26.127699Z","shell.execute_reply":"2024-11-28T15:33:26.132424Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"2809"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:33:33.694635Z","iopub.execute_input":"2024-11-28T15:33:33.694993Z","iopub.status.idle":"2024-11-28T15:33:33.705722Z","shell.execute_reply.started":"2024-11-28T15:33:33.694963Z","shell.execute_reply":"2024-11-28T15:33:33.704776Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n0     i be go through some thing with my feeling and...   \n1     i be go through some thing with my feeling and...   \n2     i be go through some thing with my feeling and...   \n3     i be go through some thing with my feeling and...   \n4     i be go through some thing with my feeling and...   \n...                                                 ...   \n3507  my grandson s step  mother send he to school w...   \n3508  my boyfriend be in recovery from drug addictio...   \n3509  the birth mother attempt suicide several time ...   \n3510  i think adult life be make he depressed and we...   \n3511  i just take a job that require i to travel far...   \n\n                                               Response  \n0     if everyone think you be worthless  then maybe...  \n1     hello  and thank you for your question and see...  \n2     first thing i would suggest be get the sleep y...  \n3     therapy be essential for those that be feel de...  \n4     i first want to let you know that you be not a...  \n...                                                 ...  \n3507  absolutely not    it be never in a child s goo...  \n3508  i be sorry you have tension between you and yo...  \n3509  the true answer be   no one can really say wit...  \n3510  how do you help yourself to believe you requir...  \n3511                           hmm this be a tough one   \n\n[3512 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>if everyone think you be worthless  then maybe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>hello  and thank you for your question and see...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>first thing i would suggest be get the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>therapy be essential for those that be feel de...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i be go through some thing with my feeling and...</td>\n      <td>i first want to let you know that you be not a...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3507</th>\n      <td>my grandson s step  mother send he to school w...</td>\n      <td>absolutely not    it be never in a child s goo...</td>\n    </tr>\n    <tr>\n      <th>3508</th>\n      <td>my boyfriend be in recovery from drug addictio...</td>\n      <td>i be sorry you have tension between you and yo...</td>\n    </tr>\n    <tr>\n      <th>3509</th>\n      <td>the birth mother attempt suicide several time ...</td>\n      <td>the true answer be   no one can really say wit...</td>\n    </tr>\n    <tr>\n      <th>3510</th>\n      <td>i think adult life be make he depressed and we...</td>\n      <td>how do you help yourself to believe you requir...</td>\n    </tr>\n    <tr>\n      <th>3511</th>\n      <td>i just take a job that require i to travel far...</td>\n      <td>hmm this be a tough one</td>\n    </tr>\n  </tbody>\n</table>\n<p>3512 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:29:13.041136Z","iopub.execute_input":"2024-11-28T15:29:13.041864Z","iopub.status.idle":"2024-11-28T15:29:13.051111Z","shell.execute_reply.started":"2024-11-28T15:29:13.041805Z","shell.execute_reply":"2024-11-28T15:29:13.050296Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n0     I be go through some thing with my feeling and...   \n1     I be go through some thing with my feeling and...   \n2     I be go through some thing with my feeling and...   \n3     I be go through some thing with my feeling and...   \n4     I be go through some thing with my feeling and...   \n...                                                 ...   \n3507  my grandson 's step - mother send he to school...   \n3508  my boyfriend be in recovery from drug addictio...   \n3509  the birth mother attempt suicide several time ...   \n3510  I think adult life be make he depressed and we...   \n3511  I just take a job that require I to travel far...   \n\n                                               Response  \n0     if everyone think you be worthless , then mayb...  \n1     hello , and thank you for your question and se...  \n2     first thing I would suggest be get the sleep y...  \n3     therapy be essential for those that be feel de...  \n4     I first want to let you know that you be not a...  \n...                                                 ...  \n3507  absolutely not !   it be never in a child 's g...  \n3508  I be sorry you have tension between you and yo...  \n3509  the true answer be , \" no one can really say w...  \n3510  how do you help yourself to believe you requir...  \n3511                          hmm this be a tough one !  \n\n[3512 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>if everyone think you be worthless , then mayb...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>hello , and thank you for your question and se...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>first thing I would suggest be get the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>therapy be essential for those that be feel de...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I be go through some thing with my feeling and...</td>\n      <td>I first want to let you know that you be not a...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3507</th>\n      <td>my grandson 's step - mother send he to school...</td>\n      <td>absolutely not !   it be never in a child 's g...</td>\n    </tr>\n    <tr>\n      <th>3508</th>\n      <td>my boyfriend be in recovery from drug addictio...</td>\n      <td>I be sorry you have tension between you and yo...</td>\n    </tr>\n    <tr>\n      <th>3509</th>\n      <td>the birth mother attempt suicide several time ...</td>\n      <td>the true answer be , \" no one can really say w...</td>\n    </tr>\n    <tr>\n      <th>3510</th>\n      <td>I think adult life be make he depressed and we...</td>\n      <td>how do you help yourself to believe you requir...</td>\n    </tr>\n    <tr>\n      <th>3511</th>\n      <td>I just take a job that require I to travel far...</td>\n      <td>hmm this be a tough one !</td>\n    </tr>\n  </tbody>\n</table>\n<p>3512 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:33:40.855727Z","iopub.execute_input":"2024-11-28T15:33:40.856084Z","iopub.status.idle":"2024-11-28T15:33:40.860213Z","shell.execute_reply.started":"2024-11-28T15:33:40.856054Z","shell.execute_reply":"2024-11-28T15:33:40.859375Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"def tokenize_function(row):\n    # Tokenize the conversations\n    row['input_ids'] = tokenizer(row[\"Context\"], padding=\"max_length\", truncation=True, max_length = 128, return_tensors=\"pt\").input_ids[0]\n\n    # Assuming \"answer\" column is already a string, no need for conversion\n    row['labels'] = tokenizer(row[\"Response\"], padding=\"max_length\", truncation=True, max_length = 128, return_tensors=\"pt\").input_ids[0]\n\n    return row\n\n\n\ntokenized_df = train_df.apply(tokenize_function, axis=1)\ntokenized_df['input_ids'] = tokenized_df['input_ids'].apply(lambda x: x.tolist())\ntokenized_df['labels'] = tokenized_df['labels'].apply(lambda x: x.tolist())\n\n\ntokenized_df2 = test_df.apply(tokenize_function, axis=1)\ntokenized_df2['input_ids'] = tokenized_df2['input_ids'].apply(lambda x: x.tolist())\ntokenized_df2['labels'] = tokenized_df2['labels'].apply(lambda x: x.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:33:47.519041Z","iopub.execute_input":"2024-11-28T15:33:47.519374Z","iopub.status.idle":"2024-11-28T15:33:56.224178Z","shell.execute_reply.started":"2024-11-28T15:33:47.519344Z","shell.execute_reply":"2024-11-28T15:33:56.223135Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"tokenized_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:18.199361Z","iopub.execute_input":"2024-11-28T15:34:18.200048Z","iopub.status.idle":"2024-11-28T15:34:18.223194Z","shell.execute_reply.started":"2024-11-28T15:34:18.200010Z","shell.execute_reply":"2024-11-28T15:34:18.222322Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n2659  we do not have sex a lot  i cheat when we argu...   \n1057  i do not know how to notice or express my feel...   \n2111  i start counsel  therapy in a few day  i be fr...   \n1565  i do not know how to tell someone how i feel a...   \n631   my daughter seem to be develop at a normal rat...   \n...                                                 ...   \n1130  i go to my ex  boyfriend to reach out to one o...   \n1294  last year  i just always feel hopeless  i do n...   \n860   i recently go through a divorce  my ex  husban...   \n3507  my grandson s step  mother send he to school w...   \n3174  cheating be something unacceptable for i but b...   \n\n                                               Response  \\\n2659   hello  and thank you for your question  the q...   \n1057  well  then give yourself some credit for notic...   \n2111  people do cry in therapy sometimes  but it be ...   \n1565   practice make perfectsimply by express yourse...   \n631   good for you to know your daughter s friendshi...   \n...                                                 ...   \n1130  your compassionate reach out to the friend be ...   \n1294  i be so sorry about your loss    lose someone ...   \n860   unfortunately  i can not tell you what your si...   \n3507  absolutely not    it be never in a child s goo...   \n3174  it be completely understandable that you be st...   \n\n                                              input_ids  \\\n2659  [128000, 906, 656, 539, 617, 1877, 264, 2763, ...   \n1057  [128000, 72, 656, 539, 1440, 1268, 311, 5406, ...   \n2111  [128000, 72, 1212, 16467, 220, 15419, 304, 264...   \n1565  [128000, 72, 656, 539, 1440, 1268, 311, 3371, ...   \n631   [128000, 2465, 10003, 2873, 311, 387, 2274, 52...   \n...                                                 ...   \n1130  [128000, 72, 733, 311, 856, 506, 220, 26923, 3...   \n1294  [128000, 4354, 1060, 220, 602, 1120, 2744, 273...   \n860   [128000, 72, 6051, 733, 1555, 264, 25549, 220,...   \n3507  [128000, 2465, 66955, 274, 3094, 220, 6691, 37...   \n3174  [128000, 1557, 1113, 387, 2555, 44085, 369, 60...   \n\n                                                 labels  \n2659  [128000, 24748, 220, 323, 9901, 499, 369, 701,...  \n1057  [128000, 9336, 220, 1243, 3041, 6261, 1063, 68...  \n2111  [128000, 16455, 656, 16106, 304, 15419, 7170, ...  \n1565  [128000, 6725, 1304, 4832, 15124, 2603, 555, 3...  \n631   [128000, 19045, 369, 499, 311, 1440, 701, 1000...  \n...                                                 ...  \n1130  [128000, 22479, 60961, 5662, 704, 311, 279, 43...  \n1294  [128000, 72, 387, 779, 14931, 922, 701, 4814, ...  \n860   [128000, 359, 11275, 220, 602, 649, 539, 3371,...  \n3507  [128000, 3518, 9887, 539, 256, 4194, 433, 387,...  \n3174  [128000, 275, 387, 6724, 49839, 430, 499, 387,...  \n\n[2809 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n      <th>input_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2659</th>\n      <td>we do not have sex a lot  i cheat when we argu...</td>\n      <td>hello  and thank you for your question  the q...</td>\n      <td>[128000, 906, 656, 539, 617, 1877, 264, 2763, ...</td>\n      <td>[128000, 24748, 220, 323, 9901, 499, 369, 701,...</td>\n    </tr>\n    <tr>\n      <th>1057</th>\n      <td>i do not know how to notice or express my feel...</td>\n      <td>well  then give yourself some credit for notic...</td>\n      <td>[128000, 72, 656, 539, 1440, 1268, 311, 5406, ...</td>\n      <td>[128000, 9336, 220, 1243, 3041, 6261, 1063, 68...</td>\n    </tr>\n    <tr>\n      <th>2111</th>\n      <td>i start counsel  therapy in a few day  i be fr...</td>\n      <td>people do cry in therapy sometimes  but it be ...</td>\n      <td>[128000, 72, 1212, 16467, 220, 15419, 304, 264...</td>\n      <td>[128000, 16455, 656, 16106, 304, 15419, 7170, ...</td>\n    </tr>\n    <tr>\n      <th>1565</th>\n      <td>i do not know how to tell someone how i feel a...</td>\n      <td>practice make perfectsimply by express yourse...</td>\n      <td>[128000, 72, 656, 539, 1440, 1268, 311, 3371, ...</td>\n      <td>[128000, 6725, 1304, 4832, 15124, 2603, 555, 3...</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>my daughter seem to be develop at a normal rat...</td>\n      <td>good for you to know your daughter s friendshi...</td>\n      <td>[128000, 2465, 10003, 2873, 311, 387, 2274, 52...</td>\n      <td>[128000, 19045, 369, 499, 311, 1440, 701, 1000...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1130</th>\n      <td>i go to my ex  boyfriend to reach out to one o...</td>\n      <td>your compassionate reach out to the friend be ...</td>\n      <td>[128000, 72, 733, 311, 856, 506, 220, 26923, 3...</td>\n      <td>[128000, 22479, 60961, 5662, 704, 311, 279, 43...</td>\n    </tr>\n    <tr>\n      <th>1294</th>\n      <td>last year  i just always feel hopeless  i do n...</td>\n      <td>i be so sorry about your loss    lose someone ...</td>\n      <td>[128000, 4354, 1060, 220, 602, 1120, 2744, 273...</td>\n      <td>[128000, 72, 387, 779, 14931, 922, 701, 4814, ...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>i recently go through a divorce  my ex  husban...</td>\n      <td>unfortunately  i can not tell you what your si...</td>\n      <td>[128000, 72, 6051, 733, 1555, 264, 25549, 220,...</td>\n      <td>[128000, 359, 11275, 220, 602, 649, 539, 3371,...</td>\n    </tr>\n    <tr>\n      <th>3507</th>\n      <td>my grandson s step  mother send he to school w...</td>\n      <td>absolutely not    it be never in a child s goo...</td>\n      <td>[128000, 2465, 66955, 274, 3094, 220, 6691, 37...</td>\n      <td>[128000, 3518, 9887, 539, 256, 4194, 433, 387,...</td>\n    </tr>\n    <tr>\n      <th>3174</th>\n      <td>cheating be something unacceptable for i but b...</td>\n      <td>it be completely understandable that you be st...</td>\n      <td>[128000, 1557, 1113, 387, 2555, 44085, 369, 60...</td>\n      <td>[128000, 275, 387, 6724, 49839, 430, 499, 387,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2809 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"tokenized_df2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:25.580251Z","iopub.execute_input":"2024-11-28T15:34:25.581050Z","iopub.status.idle":"2024-11-28T15:34:25.602324Z","shell.execute_reply.started":"2024-11-28T15:34:25.581016Z","shell.execute_reply":"2024-11-28T15:34:25.601340Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"                                                Context  \\\n495   i have hit my head on wall and floor ever sinc...   \n1592  over a year ago i have a female friend  she tu...   \n2314  my long  distance girlfriend be in a sorority ...   \n1475  cheating be something unacceptable for i but b...   \n2772  i have twin toddler  i experience a death of l...   \n...                                                 ...   \n1210  we have be together over a year  we spend time...   \n1748  i read that you should ignore they and they ha...   \n227   i be depressed  i have be for year  i hide it ...   \n555   i be violently rape by another woman who be my...   \n2864  i ’m try to make marriage work after a split  ...   \n\n                                               Response  \\\n495   the good way to handle anxiety of this level b...   \n1592  we woman really do tend to struggle with the c...   \n2314  you may already be do as much as possible for ...   \n1475  it be completely understandable that you be st...   \n2772  first  let i say that you be a survivor and a ...   \n...                                                 ...   \n1210  hello  and thank you for your question  i be v...   \n1748  it be not correct because someone who be narci...   \n227   hi georgia  there be a really good lesson here...   \n555   i be sorry for your suffering  there be therap...   \n2864  be you upset  be the more pertinent question  ...   \n\n                                              input_ids  \\\n495   [128000, 72, 617, 4295, 856, 2010, 389, 7147, ...   \n1592  [128000, 2017, 264, 1060, 4227, 602, 617, 264,...   \n2314  [128000, 2465, 1317, 220, 6138, 23601, 387, 30...   \n1475  [128000, 1557, 1113, 387, 2555, 44085, 369, 60...   \n2772  [128000, 72, 617, 28497, 52335, 220, 602, 3217...   \n...                                                 ...   \n1210  [128000, 906, 617, 387, 3871, 927, 264, 1060, ...   \n1748  [128000, 72, 1373, 430, 499, 1288, 10240, 814,...   \n227   [128000, 72, 387, 42642, 220, 602, 617, 387, 3...   \n555   [128000, 72, 387, 65300, 17199, 555, 2500, 533...   \n2864  [128000, 72, 18217, 76, 1456, 311, 1304, 11103...   \n\n                                                 labels  \n495   [128000, 1820, 1695, 1648, 311, 3790, 18547, 3...  \n1592  [128000, 906, 5333, 2216, 656, 8541, 311, 1499...  \n2314  [128000, 9514, 1253, 2736, 387, 656, 439, 1790...  \n1475  [128000, 275, 387, 6724, 49839, 430, 499, 387,...  \n2772  [128000, 3983, 220, 1095, 602, 2019, 430, 499,...  \n...                                                 ...  \n1210  [128000, 15339, 220, 323, 9901, 499, 369, 701,...  \n1748  [128000, 275, 387, 539, 4495, 1606, 4423, 889,...  \n227   [128000, 6151, 3980, 48049, 220, 1070, 387, 26...  \n555   [128000, 72, 387, 14931, 369, 701, 16066, 220,...  \n2864  [128000, 1395, 499, 23268, 220, 387, 279, 810,...  \n\n[703 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n      <th>input_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>495</th>\n      <td>i have hit my head on wall and floor ever sinc...</td>\n      <td>the good way to handle anxiety of this level b...</td>\n      <td>[128000, 72, 617, 4295, 856, 2010, 389, 7147, ...</td>\n      <td>[128000, 1820, 1695, 1648, 311, 3790, 18547, 3...</td>\n    </tr>\n    <tr>\n      <th>1592</th>\n      <td>over a year ago i have a female friend  she tu...</td>\n      <td>we woman really do tend to struggle with the c...</td>\n      <td>[128000, 2017, 264, 1060, 4227, 602, 617, 264,...</td>\n      <td>[128000, 906, 5333, 2216, 656, 8541, 311, 1499...</td>\n    </tr>\n    <tr>\n      <th>2314</th>\n      <td>my long  distance girlfriend be in a sorority ...</td>\n      <td>you may already be do as much as possible for ...</td>\n      <td>[128000, 2465, 1317, 220, 6138, 23601, 387, 30...</td>\n      <td>[128000, 9514, 1253, 2736, 387, 656, 439, 1790...</td>\n    </tr>\n    <tr>\n      <th>1475</th>\n      <td>cheating be something unacceptable for i but b...</td>\n      <td>it be completely understandable that you be st...</td>\n      <td>[128000, 1557, 1113, 387, 2555, 44085, 369, 60...</td>\n      <td>[128000, 275, 387, 6724, 49839, 430, 499, 387,...</td>\n    </tr>\n    <tr>\n      <th>2772</th>\n      <td>i have twin toddler  i experience a death of l...</td>\n      <td>first  let i say that you be a survivor and a ...</td>\n      <td>[128000, 72, 617, 28497, 52335, 220, 602, 3217...</td>\n      <td>[128000, 3983, 220, 1095, 602, 2019, 430, 499,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1210</th>\n      <td>we have be together over a year  we spend time...</td>\n      <td>hello  and thank you for your question  i be v...</td>\n      <td>[128000, 906, 617, 387, 3871, 927, 264, 1060, ...</td>\n      <td>[128000, 15339, 220, 323, 9901, 499, 369, 701,...</td>\n    </tr>\n    <tr>\n      <th>1748</th>\n      <td>i read that you should ignore they and they ha...</td>\n      <td>it be not correct because someone who be narci...</td>\n      <td>[128000, 72, 1373, 430, 499, 1288, 10240, 814,...</td>\n      <td>[128000, 275, 387, 539, 4495, 1606, 4423, 889,...</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>i be depressed  i have be for year  i hide it ...</td>\n      <td>hi georgia  there be a really good lesson here...</td>\n      <td>[128000, 72, 387, 42642, 220, 602, 617, 387, 3...</td>\n      <td>[128000, 6151, 3980, 48049, 220, 1070, 387, 26...</td>\n    </tr>\n    <tr>\n      <th>555</th>\n      <td>i be violently rape by another woman who be my...</td>\n      <td>i be sorry for your suffering  there be therap...</td>\n      <td>[128000, 72, 387, 65300, 17199, 555, 2500, 533...</td>\n      <td>[128000, 72, 387, 14931, 369, 701, 16066, 220,...</td>\n    </tr>\n    <tr>\n      <th>2864</th>\n      <td>i ’m try to make marriage work after a split  ...</td>\n      <td>be you upset  be the more pertinent question  ...</td>\n      <td>[128000, 72, 18217, 76, 1456, 311, 1304, 11103...</td>\n      <td>[128000, 1395, 499, 23268, 220, 387, 279, 810,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>703 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"tokenized_df.drop(columns=[\"Context\",\"Response\"], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:31.326831Z","iopub.execute_input":"2024-11-28T15:34:31.327494Z","iopub.status.idle":"2024-11-28T15:34:31.346819Z","shell.execute_reply.started":"2024-11-28T15:34:31.327460Z","shell.execute_reply":"2024-11-28T15:34:31.346179Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"tokenized_df2.drop(columns=[\"Context\",\"Response\"], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:36.626423Z","iopub.execute_input":"2024-11-28T15:34:36.626782Z","iopub.status.idle":"2024-11-28T15:34:36.636248Z","shell.execute_reply.started":"2024-11-28T15:34:36.626751Z","shell.execute_reply":"2024-11-28T15:34:36.635496Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"tokenized_df2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:40.374261Z","iopub.execute_input":"2024-11-28T15:34:40.375134Z","iopub.status.idle":"2024-11-28T15:34:40.395408Z","shell.execute_reply.started":"2024-11-28T15:34:40.375099Z","shell.execute_reply":"2024-11-28T15:34:40.394556Z"}},"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"                                              input_ids  \\\n495   [128000, 72, 617, 4295, 856, 2010, 389, 7147, ...   \n1592  [128000, 2017, 264, 1060, 4227, 602, 617, 264,...   \n2314  [128000, 2465, 1317, 220, 6138, 23601, 387, 30...   \n1475  [128000, 1557, 1113, 387, 2555, 44085, 369, 60...   \n2772  [128000, 72, 617, 28497, 52335, 220, 602, 3217...   \n...                                                 ...   \n1210  [128000, 906, 617, 387, 3871, 927, 264, 1060, ...   \n1748  [128000, 72, 1373, 430, 499, 1288, 10240, 814,...   \n227   [128000, 72, 387, 42642, 220, 602, 617, 387, 3...   \n555   [128000, 72, 387, 65300, 17199, 555, 2500, 533...   \n2864  [128000, 72, 18217, 76, 1456, 311, 1304, 11103...   \n\n                                                 labels  \n495   [128000, 1820, 1695, 1648, 311, 3790, 18547, 3...  \n1592  [128000, 906, 5333, 2216, 656, 8541, 311, 1499...  \n2314  [128000, 9514, 1253, 2736, 387, 656, 439, 1790...  \n1475  [128000, 275, 387, 6724, 49839, 430, 499, 387,...  \n2772  [128000, 3983, 220, 1095, 602, 2019, 430, 499,...  \n...                                                 ...  \n1210  [128000, 15339, 220, 323, 9901, 499, 369, 701,...  \n1748  [128000, 275, 387, 539, 4495, 1606, 4423, 889,...  \n227   [128000, 6151, 3980, 48049, 220, 1070, 387, 26...  \n555   [128000, 72, 387, 14931, 369, 701, 16066, 220,...  \n2864  [128000, 1395, 499, 23268, 220, 387, 279, 810,...  \n\n[703 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>495</th>\n      <td>[128000, 72, 617, 4295, 856, 2010, 389, 7147, ...</td>\n      <td>[128000, 1820, 1695, 1648, 311, 3790, 18547, 3...</td>\n    </tr>\n    <tr>\n      <th>1592</th>\n      <td>[128000, 2017, 264, 1060, 4227, 602, 617, 264,...</td>\n      <td>[128000, 906, 5333, 2216, 656, 8541, 311, 1499...</td>\n    </tr>\n    <tr>\n      <th>2314</th>\n      <td>[128000, 2465, 1317, 220, 6138, 23601, 387, 30...</td>\n      <td>[128000, 9514, 1253, 2736, 387, 656, 439, 1790...</td>\n    </tr>\n    <tr>\n      <th>1475</th>\n      <td>[128000, 1557, 1113, 387, 2555, 44085, 369, 60...</td>\n      <td>[128000, 275, 387, 6724, 49839, 430, 499, 387,...</td>\n    </tr>\n    <tr>\n      <th>2772</th>\n      <td>[128000, 72, 617, 28497, 52335, 220, 602, 3217...</td>\n      <td>[128000, 3983, 220, 1095, 602, 2019, 430, 499,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1210</th>\n      <td>[128000, 906, 617, 387, 3871, 927, 264, 1060, ...</td>\n      <td>[128000, 15339, 220, 323, 9901, 499, 369, 701,...</td>\n    </tr>\n    <tr>\n      <th>1748</th>\n      <td>[128000, 72, 1373, 430, 499, 1288, 10240, 814,...</td>\n      <td>[128000, 275, 387, 539, 4495, 1606, 4423, 889,...</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>[128000, 72, 387, 42642, 220, 602, 617, 387, 3...</td>\n      <td>[128000, 6151, 3980, 48049, 220, 1070, 387, 26...</td>\n    </tr>\n    <tr>\n      <th>555</th>\n      <td>[128000, 72, 387, 65300, 17199, 555, 2500, 533...</td>\n      <td>[128000, 72, 387, 14931, 369, 701, 16066, 220,...</td>\n    </tr>\n    <tr>\n      <th>2864</th>\n      <td>[128000, 72, 18217, 76, 1456, 311, 1304, 11103...</td>\n      <td>[128000, 1395, 499, 23268, 220, 387, 279, 810,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>703 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"from datasets import Dataset\n\n# # Assuming `tokenized_df` is your pandas DataFrame\n# train_dataset = Dataset.from_pandas(train_df)\n# test_dataset = Dataset.from_pandas(test_df)\n\ntraining_dataset = Dataset.from_pandas(tokenized_df)\ntest_dataset = Dataset.from_pandas(tokenized_df2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:48.966383Z","iopub.execute_input":"2024-11-28T15:34:48.967060Z","iopub.status.idle":"2024-11-28T15:34:49.069137Z","shell.execute_reply.started":"2024-11-28T15:34:48.967025Z","shell.execute_reply":"2024-11-28T15:34:49.068433Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"print(training_dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:53.461725Z","iopub.execute_input":"2024-11-28T15:34:53.462087Z","iopub.status.idle":"2024-11-28T15:34:53.466645Z","shell.execute_reply.started":"2024-11-28T15:34:53.462053Z","shell.execute_reply":"2024-11-28T15:34:53.465862Z"}},"outputs":[{"name":"stdout","text":"['input_ids', 'labels', '__index_level_0__']\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"print(test_dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:34:58.540740Z","iopub.execute_input":"2024-11-28T15:34:58.541391Z","iopub.status.idle":"2024-11-28T15:34:58.545871Z","shell.execute_reply.started":"2024-11-28T15:34:58.541359Z","shell.execute_reply":"2024-11-28T15:34:58.545049Z"}},"outputs":[{"name":"stdout","text":"['input_ids', 'labels', '__index_level_0__']\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# Hapus kolom '__index_level_0__' setelah konversi menjadi Dataset\ntraining_dataset = training_dataset.remove_columns([\"__index_level_0__\"])\ntest_dataset = test_dataset.remove_columns([\"__index_level_0__\"])\n\n# Cek kolom yang tersisa\nprint(training_dataset.column_names)  # Pastikan hanya ada 'input_ids' dan 'labels'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:35:06.043549Z","iopub.execute_input":"2024-11-28T15:35:06.043919Z","iopub.status.idle":"2024-11-28T15:35:06.053304Z","shell.execute_reply.started":"2024-11-28T15:35:06.043887Z","shell.execute_reply":"2024-11-28T15:35:06.052437Z"}},"outputs":[{"name":"stdout","text":"['input_ids', 'labels']\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"print(test_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:35:23.216733Z","iopub.execute_input":"2024-11-28T15:35:23.217121Z","iopub.status.idle":"2024-11-28T15:35:23.222369Z","shell.execute_reply.started":"2024-11-28T15:35:23.217091Z","shell.execute_reply":"2024-11-28T15:35:23.221321Z"}},"outputs":[{"name":"stdout","text":"['input_ids', 'labels']\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"# Load LoRA configuration\npeft_args = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=8,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:35:26.818849Z","iopub.execute_input":"2024-11-28T15:35:26.819693Z","iopub.status.idle":"2024-11-28T15:35:26.823761Z","shell.execute_reply.started":"2024-11-28T15:35:26.819660Z","shell.execute_reply":"2024-11-28T15:35:26.822875Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"# Set training parameters\ntraining_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=20,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=1,\n    evaluation_strategy=\"epoch\",\n    optim=\"paged_adamw_32bit\",\n    save_steps=500,\n    logging_steps=100,\n    learning_rate=6e-5,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n#     max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:35:37.856746Z","iopub.execute_input":"2024-11-28T15:35:37.857109Z","iopub.status.idle":"2024-11-28T15:35:37.898193Z","shell.execute_reply.started":"2024-11-28T15:35:37.857078Z","shell.execute_reply":"2024-11-28T15:35:37.897375Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:35:44.511068Z","iopub.execute_input":"2024-11-28T15:35:44.511750Z","iopub.status.idle":"2024-11-28T15:35:44.519201Z","shell.execute_reply.started":"2024-11-28T15:35:44.511716Z","shell.execute_reply":"2024-11-28T15:35:44.518347Z"}},"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"# # Set supervised fine-tuning parameters\n# trainer = SFTTrainer(\n#     model=model,\n#     train_dataset=training_dataset,\n#     eval_dataset=test_dataset,\n#     peft_config=peft_args,\n#     dataset_text_field=\"text\",\n# #     max_seq_length=256,\n#     max_seq_length=3584,\n#     tokenizer=tokenizer,\n#     args=training_params,\n#     packing=False,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:16:06.387397Z","iopub.status.idle":"2024-11-28T15:16:06.387858Z","shell.execute_reply.started":"2024-11-28T15:16:06.387599Z","shell.execute_reply":"2024-11-28T15:16:06.387621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom torch.amp import GradScaler\n\n# Configure SFT parameters using SFTConfig\nsft_config = SFTConfig(\n    output_dir=\"./results\",\n    dataset_text_field=\"text\",\n    max_seq_length= None  # Set desired max sequence length here\n)\n\n# Initialize the SFTTrainer with the config\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=training_dataset,\n    eval_dataset=test_dataset,  # Uncomment if you need evaluation\n    peft_config=peft_args,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False  # Pass config object instead of individual parameters\n)\n\n# Update GradScaler to new syntax\nscaler = GradScaler()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:01.008610Z","iopub.execute_input":"2024-11-28T15:36:01.008964Z","iopub.status.idle":"2024-11-28T15:36:01.571882Z","shell.execute_reply.started":"2024-11-28T15:36:01.008934Z","shell.execute_reply":"2024-11-28T15:36:01.570988Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=training_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_args,\n    dataset_text_field=\"text\",\n#     max_seq_length=256,\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:14.843460Z","iopub.execute_input":"2024-11-28T15:36:14.843774Z","iopub.status.idle":"2024-11-28T15:36:15.007802Z","shell.execute_reply.started":"2024-11-28T15:36:14.843748Z","shell.execute_reply":"2024-11-28T15:36:15.006875Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:18.754486Z","iopub.execute_input":"2024-11-28T15:36:18.755225Z","iopub.status.idle":"2024-11-28T15:36:18.769728Z","shell.execute_reply.started":"2024-11-28T15:36:18.755191Z","shell.execute_reply":"2024-11-28T15:36:18.768865Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:20.908591Z","iopub.execute_input":"2024-11-28T15:36:20.909390Z","iopub.status.idle":"2024-11-28T15:36:20.914704Z","shell.execute_reply.started":"2024-11-28T15:36:20.909355Z","shell.execute_reply":"2024-11-28T15:36:20.913763Z"}},"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7a71a61c6290>"},"metadata":{}}],"execution_count":97},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:23.910509Z","iopub.execute_input":"2024-11-28T15:36:23.911297Z","iopub.status.idle":"2024-11-28T15:36:25.841175Z","shell.execute_reply.started":"2024-11-28T15:36:23.911258Z","shell.execute_reply":"2024-11-28T15:36:25.839536Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:176\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m inputs, module_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_kwargs:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:198\u001b[0m, in \u001b[0;36mDataParallel.scatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    194\u001b[0m     inputs: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m    195\u001b[0m     kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    196\u001b[0m     device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]],\n\u001b[1;32m    197\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:78\u001b[0m, in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Scatter with support for kwargs dictionary.\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m scattered_inputs \u001b[38;5;241m=\u001b[39m scatter(inputs, target_gpus, dim) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m---> 78\u001b[0m scattered_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scattered_inputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_kwargs):\n\u001b[1;32m     80\u001b[0m     scattered_inputs\u001b[38;5;241m.\u001b[39mextend(() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scattered_kwargs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_inputs)))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:64\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     scatter_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:55\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [obj \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:51\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:47\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_map\u001b[39m(obj):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mScatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(obj):\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:96\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     streams \u001b[38;5;241m=\u001b[39m [_get_stream(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)) \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n\u001b[0;32m---> 96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/comm.py:188\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mRuntimeError\u001b[0m: chunk expects at least a 1-dimensional tensor"],"ename":"RuntimeError","evalue":"chunk expects at least a 1-dimensional tensor","output_type":"error"}],"execution_count":98},{"cell_type":"code","source":"trainer.model.save_pretrained(\"llama-3-8B-therapist\")\ntokenizer.save_pretrained(\"llama-3-8B-therapist\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:33.830493Z","iopub.execute_input":"2024-11-28T15:36:33.831396Z","iopub.status.idle":"2024-11-28T15:36:34.264797Z","shell.execute_reply.started":"2024-11-28T15:36:33.831357Z","shell.execute_reply":"2024-11-28T15:36:34.263869Z"}},"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"('llama-3-8B-therapist/tokenizer_config.json',\n 'llama-3-8B-therapist/special_tokens_map.json',\n 'llama-3-8B-therapist/tokenizer.json')"},"metadata":{}}],"execution_count":99},{"cell_type":"code","source":"trainer.model.save_pretrained(\"/kaggle/working/llama-3-8B-therapist_MODEL\")\n\n# Save the tokenizer to the desired directory\ntokenizer.save_pretrained(\"/kaggle/working/llama-3-8B-therapist\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:16:06.399711Z","iopub.status.idle":"2024-11-28T15:16:06.400018Z","shell.execute_reply.started":"2024-11-28T15:16:06.399877Z","shell.execute_reply":"2024-11-28T15:16:06.399892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def single_inference(question):\n    messages = [\n        {\"role\": \"system\", \"content\": \"Reply to the following inquiry.\"},\n    ]\n\n    messages.append({\"role\": \"user\", \"content\": question})\n\n\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    terminators = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n\n    outputs = model.generate(\n        input_ids,\n        max_new_tokens=256,\n        eos_token_id=terminators,\n        do_sample=True,\n        temperature=0.01,\n    #     top_p=0.9,\n    )\n    response = outputs[0][input_ids.shape[-1]:]\n    output = tokenizer.decode(response, skip_special_tokens=True)\n    return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:40.005704Z","iopub.execute_input":"2024-11-28T15:36:40.006053Z","iopub.status.idle":"2024-11-28T15:36:40.011547Z","shell.execute_reply.started":"2024-11-28T15:36:40.006025Z","shell.execute_reply":"2024-11-28T15:36:40.010698Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"question = \"i am not happy with my score in university\"\n\nanswer = single_inference(question)\n\nprint(f'INPUT QUESTION:\\n{question}')\nprint(f'\\n\\nModel Answer:\\n{answer}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:36:43.765489Z","iopub.execute_input":"2024-11-28T15:36:43.765803Z","iopub.status.idle":"2024-11-28T15:37:13.137718Z","shell.execute_reply.started":"2024-11-28T15:36:43.765777Z","shell.execute_reply":"2024-11-28T15:37:13.136859Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"INPUT QUESTION:\ni am not happy with my score in university\n\n\nModel Answer:\nIt's completely normal to feel unhappy with your university score, especially if you had high expectations or felt that you didn't perform to the best of your abilities.\n\nHere are some suggestions that might help:\n\n1. **Talk to your professors or academic advisor**: They can provide you with feedback on your performance and suggest areas where you can improve. They may also be able to offer additional support or resources to help you succeed.\n2. **Reflect on your study habits**: Take some time to think about how you studied and what worked (or didn't work) for you. Consider adjusting your study habits, such as creating a schedule, using flashcards, or joining a study group.\n3. **Seek help from a tutor or mentor**: If you're struggling with a particular subject, consider hiring a tutor or finding a mentor who can provide guidance and support.\n4. **Focus on your strengths**: While it's easy to get caught up in your weaknesses, try to focus on your strengths and the things you're doing well. This can help boost your confidence and motivation.\n5. **Develop a growth mindset**: Remember that your university score is not a definitive measure of your worth or potential. You can always learn and grow from your experiences, and there are many paths to success.\n\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.backends.cuda)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:16:06.404298Z","iopub.status.idle":"2024-11-28T15:16:06.404562Z","shell.execute_reply.started":"2024-11-28T15:16:06.404436Z","shell.execute_reply":"2024-11-28T15:16:06.404449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True if CUDA is available\n# print(torch.cuda.current_device())  # Prints the current GPU device (if available)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:16:06.406044Z","iopub.status.idle":"2024-11-28T15:16:06.406325Z","shell.execute_reply.started":"2024-11-28T15:16:06.406192Z","shell.execute_reply":"2024-11-28T15:16:06.406206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T15:16:06.406999Z","iopub.status.idle":"2024-11-28T15:16:06.407262Z","shell.execute_reply.started":"2024-11-28T15:16:06.407131Z","shell.execute_reply":"2024-11-28T15:16:06.407145Z"}},"outputs":[],"execution_count":null}]}